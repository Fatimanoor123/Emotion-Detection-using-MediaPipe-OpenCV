# -*- coding: utf-8 -*-
"""emotion detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YnWTNCnGkMNHiETji6vGR3Rc97Y6hC93
"""

# ==============================
# Emotion Detection — MediaPipe + OpenCV
# ==============================
import cv2
import mediapipe as mp
import numpy as np
import math
import os
import shutil
from google.colab import drive

# ----- Mount Drive -----
drive.mount('/content/drive')

# ----- Paths: UPDATE these to your files in Drive -----
input_drive_path  = "/content/drive/MyDrive/AI ML Projects/emotion detection system/Emotion detection.mp4"
output_drive_path = "/content/drive/MyDrive/AI ML Projects/emotion detection system/output_emotion.mp4"

# Write to a temp file in Colab first (reliable), then copy to Drive
temp_output = "/content/output_emotion.mp4"
os.makedirs(os.path.dirname(output_drive_path), exist_ok=True)

# ----- MediaPipe FaceMesh init -----
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,
                                  max_num_faces=1,
                                  refine_landmarks=True,
                                  min_detection_confidence=0.5,
                                  min_tracking_confidence=0.5)

# ----- Utility: Euclidean distance -----
def dist_pt(a, b):
    return math.hypot(a[0]-b[0], a[1]-b[1])

# ----- Emotion classifier (scale-invariant using interocular distance) -----
def classify_emotion(landmarks, img_w, img_h):
    # landmark indices we use (MediaPipe FaceMesh)
    # mouth corners / top / bottom
    lm_left_mouth   = landmarks[61]
    lm_right_mouth  = landmarks[291]
    lm_top_mouth    = landmarks[13]
    lm_bottom_mouth = landmarks[14]

    # eyes
    lm_left_eye_top    = landmarks[159]
    lm_left_eye_bottom = landmarks[145]
    lm_right_eye_top   = landmarks[386]
    lm_right_eye_bottom= landmarks[374]

    # reference points for scale (inter-ocular): landmarks 33 and 263 (outer eye corners)
    lm_outer_left_eye  = landmarks[33]
    lm_outer_right_eye = landmarks[263]

    # convert to pixel coords
    def px(lm):
        return int(lm.x * img_w), int(lm.y * img_h)

    left_mouth   = px(lm_left_mouth)
    right_mouth  = px(lm_right_mouth)
    top_mouth    = px(lm_top_mouth)
    bottom_mouth = px(lm_bottom_mouth)
    left_eye_top = px(lm_left_eye_top)
    left_eye_bottom = px(lm_left_eye_bottom)
    right_eye_top = px(lm_right_eye_top)
    right_eye_bottom = px(lm_right_eye_bottom)
    outer_left_eye = px(lm_outer_left_eye)
    outer_right_eye = px(lm_outer_right_eye)

    # distances
    mouth_width  = dist_pt(left_mouth, right_mouth) + 1e-6
    mouth_height = dist_pt(top_mouth, bottom_mouth) + 1e-6
    eye_open_left  = dist_pt(left_eye_top, left_eye_bottom) + 1e-6
    eye_open_right = dist_pt(right_eye_top, right_eye_bottom) + 1e-6
    interocular = dist_pt(outer_left_eye, outer_right_eye) + 1e-6

    # normalized (scale-invariant) features
    mw_n = mouth_width / interocular
    mh_n = mouth_height / interocular
    eol_n = eye_open_left / interocular
    eor_n = eye_open_right / interocular
    eye_avg_n = (eol_n + eor_n) / 2.0

    # ----- rules (tweakable thresholds) -----
    # SURPRISED: large mouth opening AND eyes open
    if mh_n > 0.20 and eye_avg_n > 0.055:
        return "Surprised"
    # HAPPY: wide mouth and moderate opening (smile)
    if mw_n > 0.46 and mh_n > 0.08:
        return "Happy "
    # SAD: small mouth opening, slightly narrowed eyes and narrower mouth
    if mh_n < 0.07 and mw_n < 0.40 and eye_avg_n < 0.035:
        return "Sad "
    # NEUTRAL: default
    return "Neutral "

# ----- Open input video -----
cap = cv2.VideoCapture(input_drive_path)
if not cap.isOpened():
    raise FileNotFoundError(f"Could not open input video: {input_drive_path}")

width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps    = int(cap.get(cv2.CAP_PROP_FPS)) or 24

fourcc = cv2.VideoWriter_fourcc(*"mp4v")
out = cv2.VideoWriter(temp_output, fourcc, fps, (width, height))

frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0
processed = 0

print(f"Processing {frame_count} frames (approx). This may take a while...")

try:
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb)

        emotion_text = "No face detected"

        if results.multi_face_landmarks and len(results.multi_face_landmarks) > 0:
            face_landmarks = results.multi_face_landmarks[0].landmark
            emotion_text = classify_emotion(face_landmarks, width, height)

            # Draw face mesh (optional)
            mp.solutions.drawing_utils.draw_landmarks(
                frame, results.multi_face_landmarks[0], mp_face_mesh.FACEMESH_TESSELATION,
                mp.solutions.drawing_utils.DrawingSpec(color=(0,255,0), thickness=1, circle_radius=1),
                mp.solutions.drawing_utils.DrawingSpec(color=(0,255,255), thickness=1)
            )

        # Prepare big dark text with white border for visibility
        text = emotion_text  # NO question marks
        font = cv2.FONT_HERSHEY_DUPLEX
        scale = 2.0
        # compute text size and center
        (text_w, text_h), _ = cv2.getTextSize(text, font, scale, thickness=4)
        x = max(10, (width - text_w) // 2)
        y = int(40 + text_h)

        # white thick layer (outline / background)
        cv2.putText(frame, text, (x, y), font, scale, (255,255,255), thickness=8, lineType=cv2.LINE_AA)
        # black top layer (dark text)
        cv2.putText(frame, text, (x, y), font, scale, (0,0,0), thickness=4, lineType=cv2.LINE_AA)

        out.write(frame)
        processed += 1
        if processed % 100 == 0:
            print(f"Processed {processed} frames...")

finally:
    cap.release()
    out.release()
    face_mesh.close()

# Copy temp output to Drive
shutil.copy(temp_output, output_drive_path)
print("✅ Done. Output copied to Drive at:")
print(output_drive_path)